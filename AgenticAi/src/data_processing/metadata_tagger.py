from typing import Dict, List
from openai import OpenAI
import os
import json
from pathlib import Path

FIXED_CATEGORIES = ["Convention", "Depot_Vente", "Guide_NGBSS", "Offres"]


def categorize_text_document(text_file_path: str) -> Dict[str, any]:
    """
    Cat√©gorise un document texte en utilisant DeepSeek.
    
    Args:
        text_file_path: Chemin vers le fichier .txt (issu du preprocessing)
        
    Returns:
        Dict avec categories, categorie_principale, partenaire, etc.
    """
    
    text_file_path = Path(text_file_path)
    file_name = text_file_path.name
    
    # Lire le contenu du fichier texte
    try:
        with open(text_file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        print(f" Erreur lecture fichier {file_name}: {e}")
        return _fallback_metadata(file_name, "")
    
    # Appel √† DeepSeek pour cat√©gorisation
    client = OpenAI(
        api_key="01Gu-xIiEQJWwuikkIdaPSSViTJLBpiUN9erLplVzCDJPErt8Qz8EcQ_t3YtzerzjpZ1wTNqof74JIYOfGBrqA",
        base_url="https://api.deepseek.com"
    )
    
    prompt = f"""Tu es un expert en classification de documents Alg√©rie T√©l√©com. Analyse attentivement ce document et d√©termine sa/ses cat√©gorie(s).

CAT√âGORIES DISPONIBLES (un document peut avoir plusieurs cat√©gories):

1. **Convention**: 
   - Accords de partenariat entre Alg√©rie T√©l√©com et entreprises
   - Conventions commerciales
   - Contrats de collaboration
   - Documents sign√©s entre partenaires (ex: Convention AT & √âtablissement X)

2. **Depot_Vente**: 
   - Documents sur les points de vente physiques
   - Proc√©dures de d√©p√¥t de vente
   - Gestion des magasins/boutiques AT
   - Inventaires, stocks de d√©p√¥ts

3. **Guide_NGBSS**: 
   - Guides techniques syst√®me NGBSS (New Generation Business Support System)
   - Proc√©dures internes op√©rationnelles
   - Documentation technique
   - Instructions √©tape par √©tape
   - Tutoriels et formations
   - Manuels d'utilisation

4. **Offres**: 
   - Offres commerciales (packs, forfaits)
   - Argumentaires de vente
   - Tarifs et grilles tarifaires
   - Promotions clients
   - Fiches produits/services (Idoom Fibre, 4G LTE, Gamers, etc.)

---

**Nom du fichier**: {file_name}

**Contenu du document** (premiers 1200 caract√®res):
{content[:1200] if content else "Pas de contenu disponible"}

---

**INSTRUCTIONS IMPORTANTES**:
- Lis ATTENTIVEMENT le CONTENU, pas seulement le nom du fichier
- Un document peut avoir PLUSIEURS cat√©gories si pertinent
- Identifie le partenaire mentionn√© (Sonatrach, Sonelgaz, √âtablissement X/Y/Z, etc.)
- Pour les offres, identifie le type (Gamers, Fibre, ADSL, 4G LTE, Locataire, Propri√©taire, etc.)
- Si c'est un guide/manuel technique ‚Üí Guide_NGBSS
- Si √ßa parle de points de vente/magasins ‚Üí Depot_Vente
- Si c'est un accord entre entreprises ‚Üí Convention
- Si c'est une offre client/tarif ‚Üí Offres

Retourne UNIQUEMENT ce JSON sans markdown:
{{
  "categories": ["categorie1", "categorie2"],
  "categorie_principale": "la_plus_pertinente",
  "partenaire": "nom_partenaire_si_trouve_sinon_Inconnu",
  "type_offre": "type_si_offre_sinon_N/A",
  "confidence": 0.95,
  "raison": "explication_detaillee_du_choix"
}}"""

    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "Tu es un expert en classification documentaire Alg√©rie T√©l√©com. Analyse le CONTENU en priorit√©. Sois pr√©cis et exhaustif."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,
            response_format={"type": "json_object"}
        )
        
        result = json.loads(response.choices[0].message.content)
        
        # Valider les cat√©gories
        result["categories"] = [c for c in result.get("categories", []) if c in FIXED_CATEGORIES]
        if not result["categories"]:
            result["categories"] = [_fallback_category(file_name, content)]
        
        if result.get("categorie_principale") not in FIXED_CATEGORIES:
            result["categorie_principale"] = result["categories"][0]
        
        result["source_document"] = file_name
        
        print(f"‚úÖ {file_name}")
        print(f"   Cat√©gories: {result['categories']}")
        print(f"   Principale: {result['categorie_principale']}")
        print(f"   Partenaire: {result['partenaire']}")
        print(f"   Type Offre: {result['type_offre']}")
        print(f"   Confidence: {result['confidence']}")
        
        return result
        
    except Exception as e:
        print(f"‚ùå Erreur DeepSeek pour {file_name}: {e}")
        return _fallback_metadata(file_name, content)


def _fallback_category(file_name: str, content: str = "") -> str:
    """Cat√©gorie par d√©faut bas√©e sur nom ET contenu"""
    name = file_name.lower()
    content_lower = content.lower() if content else ""
    
    # Analyse du nom de fichier
    if "guide" in name or "ngbss" in name or "manuel" in name or "procedure" in name or "tutoriel" in name:
        return "Guide_NGBSS"
    if "depot" in name or "vente" in name or "magasin" in name or "boutique" in name:
        return "Depot_Vente"
    if "convention" in name or "accord" in name or "partenariat" in name:
        return "Convention"
    if "offre" in name or "tarif" in name or "pack" in name or "promotion" in name or "argumentaire" in name:
        return "Offres"
    
    # Analyse du contenu si disponible
    if content_lower:
        if any(word in content_lower for word in ["guide", "proc√©dure", "manuel", "instruction", "√©tape", "tutoriel", "ngbss"]):
            return "Guide_NGBSS"
        if any(word in content_lower for word in ["d√©p√¥t", "point de vente", "magasin", "boutique", "stock"]):
            return "Depot_Vente"
        if any(word in content_lower for word in ["convention", "accord", "partenaire", "entre", "sign√©", "√©tablissement"]):
            return "Convention"
        if any(word in content_lower for word in ["offre", "tarif", "prix", "forfait", "pack", "promotion", "argumentaire", "idoom", "fibre", "gamers"]):
            return "Offres"
    
    # Par d√©faut
    return "Offres"


def _fallback_metadata(file_name: str, content: str = "") -> Dict:
    """M√©tadonn√©es par d√©faut en cas d'erreur LLM"""
    cat = _fallback_category(file_name, content)
    return {
        "source_document": file_name,
        "categories": [cat],
        "categorie_principale": cat,
        "partenaire": "Inconnu",
        "type_offre": "N/A",
        "confidence": 0.4,
        "raison": "Cat√©gorisation par fallback (erreur LLM)"
    }


def process_all_text_files(
    input_folder: str = "AgenticAi/data/processed",
    output_folder: str = "AgenticAi/data/metadata"
):
    """
    Traite tous les fichiers texte et g√©n√®re les m√©tadonn√©es via LLM.
    
    Args:
        input_folder: Dossier contenant les fichiers .txt par cat√©gorie
        output_folder: Dossier de sortie pour les m√©tadonn√©es
        
    Returns:
        Liste de tous les documents avec m√©tadonn√©es
    """
    os.makedirs(output_folder, exist_ok=True)
    
    categories = ["Convention", "Depot_Vente", "Offres", "Offres_Arabe", "Autres", "Guide_NGBSS"]
    
    all_documents = []
    
    for category in categories:
        category_path = os.path.join(input_folder, category)
        if not os.path.exists(category_path):
            print(f"‚è≠Ô∏è  Cat√©gorie ignor√©e (dossier inexistant): {category}")
            continue
        
        print(f"\n{'='*70}")
        print(f"üìÇ Processing category: {category}")
        print(f"{'='*70}")
        
        text_files = [f for f in os.listdir(category_path) if f.endswith(".txt")]
        
        if not text_files:
            print(f"‚ö†Ô∏è  Aucun fichier .txt trouv√© dans {category}")
            continue
        
        for file_name in text_files:
            file_path = os.path.join(category_path, file_name)
            
            # Cat√©goriser avec DeepSeek
            metadata = categorize_text_document(file_path)
            
            # Lire le contenu pour l'ajouter au document
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            document = {
                "metadata": metadata,
                "content": content
            }
            
            all_documents.append(document)
    
    # Sauvegarder toutes les m√©tadonn√©es
    output_path = os.path.join(output_folder, "llm_metadata.json")
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(all_documents, f, ensure_ascii=False, indent=2)
    
    print(f"\n{'='*70}")
    print(f"üéâ Total documents processed: {len(all_documents)}")
    print(f"üíæ Metadata saved to: {output_path}")
    print(f"{'='*70}")
    
    # G√©n√©rer statistiques
    stats = {
        "total": len(all_documents),
        "by_category": {},
        "by_partner": {},
        "by_offer_type": {},
        "avg_confidence": 0
    }
    
    total_confidence = 0
    
    for doc in all_documents:
        meta = doc["metadata"]
        
        # Stats par cat√©gorie
        cat = meta["categorie_principale"]
        stats["by_category"][cat] = stats["by_category"].get(cat, 0) + 1
        
        # Stats par partenaire
        partner = meta["partenaire"]
        stats["by_partner"][partner] = stats["by_partner"].get(partner, 0) + 1
        
        # Stats par type d'offre
        if meta["type_offre"] != "N/A":
            offer_type = meta["type_offre"]
            stats["by_offer_type"][offer_type] = stats["by_offer_type"].get(offer_type, 0) + 1
        
        # Confidence moyenne
        total_confidence += meta.get("confidence", 0)
    
    stats["avg_confidence"] = round(total_confidence / len(all_documents), 2) if all_documents else 0
    
    # Sauvegarder les stats
    stats_path = os.path.join(output_folder, "llm_stats.json")
    with open(stats_path, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    print(f"\nüìä STATISTICS:")
    print(f"   Total: {stats['total']}")
    print(f"   Avg Confidence: {stats['avg_confidence']}")
    print(f"\n   By Category:")
    for cat, count in stats["by_category"].items():
        print(f"      {cat}: {count}")
    print(f"\n   By Partner:")
    for partner, count in stats["by_partner"].items():
        print(f"      {partner}: {count}")
    if stats["by_offer_type"]:
        print(f"\n   By Offer Type:")
        for offer_type, count in stats["by_offer_type"].items():
            print(f"      {offer_type}: {count}")
    
    print(f"\nüìä Stats saved to: {stats_path}")
    
    return all_documents


# Test
if __name__ == "__main__":
    print("="*70)
    print("LLM METADATA GENERATION FROM TEXT FILES")
    print("="*70)
    
    documents = process_all_text_files()
    
    # Afficher un exemple
    if documents:
        print("\n" + "="*70)
        print("EXAMPLE DOCUMENT WITH LLM METADATA")
        print("="*70)
        example = documents[0]
        print(json.dumps(example["metadata"], ensure_ascii=False, indent=2))
        print(f"\nContent preview:")
        print(example["content"][:300] + "...")